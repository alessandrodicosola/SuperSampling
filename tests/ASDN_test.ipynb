{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASDN test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff-DWAXWzO3w"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "import torchvision\n",
        "from torchsummary import summary\n",
        "\n",
        "from typing import Tuple,List\n",
        "from torch import Tensor\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from typing import List"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbpvNuG0GK0N"
      },
      "source": [
        "def print_size(tensor : Tensor):\n",
        "  print(tensor.size())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Q5es_cKqzr"
      },
      "source": [
        "# Intra dense block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-4S7U7Fqg0R"
      },
      "source": [
        "# https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\n",
        "# https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb\n",
        "\n",
        "class DenseLayer(nn.Module):\n",
        "  r\"\"\"Base dense layer where input is concatenated to the output (input forwarded into :param:base ).\n",
        "\n",
        "  Given D_i-1 the input of i-th dense layer and D_i the output, \n",
        "  then D_i = [D_i-1, base(D_i-1)] and so D_i+1 = D_i and D_i+2 = [D_i+1,base(D_i+1)] therefore\n",
        "  D_i+2 = [D_i, base(D_i+1)]\n",
        "  So the output (D_i+2) is given by the recursive concatenation of all previous input and output (D_i = [ D_i-1, base(D_i-1) ] ) and the current output ( base(D_i+1) )\n",
        "\n",
        "  Args:\n",
        "    base : The base module used for creating the dense layer\n",
        "  Attributes:\n",
        "    base : The base module used for creating the dense layer\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, base : nn.Module):\n",
        "    super(DenseLayer,self).__init__()\n",
        "    self.base = base\n",
        "\n",
        "  def forward(self, input : Tensor):\n",
        "    def concat_(input):\n",
        "      return torch.cat([input,self.base(input)], dim=1)\n",
        "    \n",
        "    if input.requires_grad:\n",
        "      return torch.utils.checkpoint.checkpoint(concat_,input)\n",
        "    else:\n",
        "      return concat_(input)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOCa_u9T9hes"
      },
      "source": [
        "class IntraDenseBlock(nn.Module):\n",
        "  \"\"\" Base Intra dense block\n",
        "\n",
        "    Args:\n",
        "      in_channels: Input channels of the block\n",
        "      out_compressed_channels: Output channels of the input and output compression convolution at the beginning and at the end of the block\n",
        "      intra_layer_output_features: Output channels of each convolution inside the block\n",
        "      n_intra_layers: Number of convolutions inside the block\n",
        "    \"\"\"\n",
        "  def __init__(self, in_channels : int , out_compressed_channels : int , intra_layer_output_features : int , n_intra_layers : int ):\n",
        "    super(IntraDenseBlock,self).__init__()\n",
        "    \n",
        "    self.input_compression_layer = nn.Conv2d(in_channels=in_channels,\n",
        "                                      out_channels=out_compressed_channels,\n",
        "                                      kernel_size = 1, \n",
        "                                      stride = 1,\n",
        "                                      padding = 0)\n",
        "    \n",
        "    self.output_compression_layer = nn.Conv2d(in_channels=out_compressed_channels + n_intra_layers * intra_layer_output_features,\n",
        "                                      out_channels=out_compressed_channels,\n",
        "                                      kernel_size = 1, \n",
        "                                      stride = 1,\n",
        "                                      padding = 0)\n",
        "    \n",
        "    self.n_intra_layers = n_intra_layers\n",
        "\n",
        "    list_intra_layers = list()    \n",
        "    for n in range(n_intra_layers):\n",
        "      # e.g:\n",
        "      # n_intra_layers = 4\n",
        "      # IC = input compression\n",
        "      # OC = output compression\n",
        "      # i:| = i-th Conv\n",
        "      # IC -> 0:| -> 1:| -> 2:| -> 3:| -> [Here we have as input for OC: n_r + 4*n_g] OC \n",
        "      intra_layer_input_features = out_compressed_channels + n * intra_layer_output_features\n",
        "\n",
        "      # A sequence of Conv(kernel_size=3,output_channels=n_g) and Relu\n",
        "      list_intra_layers.append(\n",
        "          DenseLayer(\n",
        "              nn.Sequential(\n",
        "                  nn.Conv2d(in_channels   = intra_layer_input_features,\n",
        "                            out_channels  = intra_layer_output_features,\n",
        "                            kernel_size = 3,\n",
        "                            stride = 1,\n",
        "                            padding = 1),\n",
        "                  nn.ReLU(inplace=True))))\n",
        "      \n",
        "    self.intra_layers = nn.Sequential(*list_intra_layers)\n",
        "  \n",
        "  def forward(self, input : Tensor) -> Tensor:\n",
        "    compressed_input = self.input_compression_layer(input)\n",
        "    \n",
        "    if compressed_input.requires_grad:\n",
        "      # TODO: Replace 2 (segments) with a global variable\n",
        "      dense_output = torch.utils.checkpoint.checkpoint_sequential(self.intra_layers, 2, compressed_input)\n",
        "    else:\n",
        "      dense_output = self.intra_layers(compressed_input)\n",
        "    \n",
        "    compressed_output = self.output_compression_layer(dense_output)\n",
        "\n",
        "    return compressed_input + compressed_output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx9BGbABJgWJ",
        "outputId": "8a6925f1-87e5-4a54-c6e2-933750da1eb1"
      },
      "source": [
        "# TEST IntraDenseBlock\n",
        "\n",
        "\n",
        "intra_dense_block = IntraDenseBlock(64,64,64,8).to(\"cuda:0\")\n",
        "summary(intra_dense_block,input_size = (64,48,48),batch_size=32, device='cuda')\n",
        "\n",
        "intra_dense_block = IntraDenseBlock(64,32,32,4).to(\"cuda:0\")\n",
        "summary(intra_dense_block,input_size = (64,48,48),batch_size=32, device='cuda')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [32, 64, 48, 48]           4,160\n",
            "            Conv2d-2           [32, 64, 48, 48]          36,928\n",
            "              ReLU-3           [32, 64, 48, 48]               0\n",
            "        DenseLayer-4          [32, 128, 48, 48]               0\n",
            "            Conv2d-5           [32, 64, 48, 48]          73,792\n",
            "              ReLU-6           [32, 64, 48, 48]               0\n",
            "        DenseLayer-7          [32, 192, 48, 48]               0\n",
            "            Conv2d-8           [32, 64, 48, 48]         110,656\n",
            "              ReLU-9           [32, 64, 48, 48]               0\n",
            "       DenseLayer-10          [32, 256, 48, 48]               0\n",
            "           Conv2d-11           [32, 64, 48, 48]         147,520\n",
            "             ReLU-12           [32, 64, 48, 48]               0\n",
            "       DenseLayer-13          [32, 320, 48, 48]               0\n",
            "           Conv2d-14           [32, 64, 48, 48]         184,384\n",
            "             ReLU-15           [32, 64, 48, 48]               0\n",
            "       DenseLayer-16          [32, 384, 48, 48]               0\n",
            "           Conv2d-17           [32, 64, 48, 48]         221,248\n",
            "             ReLU-18           [32, 64, 48, 48]               0\n",
            "       DenseLayer-19          [32, 448, 48, 48]               0\n",
            "           Conv2d-20           [32, 64, 48, 48]         258,112\n",
            "             ReLU-21           [32, 64, 48, 48]               0\n",
            "       DenseLayer-22          [32, 512, 48, 48]               0\n",
            "           Conv2d-23           [32, 64, 48, 48]         294,976\n",
            "             ReLU-24           [32, 64, 48, 48]               0\n",
            "       DenseLayer-25          [32, 576, 48, 48]               0\n",
            "           Conv2d-26           [32, 64, 48, 48]          36,928\n",
            "================================================================\n",
            "Total params: 1,368,704\n",
            "Trainable params: 1,368,704\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 18.00\n",
            "Forward/backward pass size (MB): 2232.00\n",
            "Params size (MB): 5.22\n",
            "Estimated Total Size (MB): 2255.22\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [32, 32, 48, 48]           2,080\n",
            "            Conv2d-2           [32, 32, 48, 48]           9,248\n",
            "              ReLU-3           [32, 32, 48, 48]               0\n",
            "        DenseLayer-4           [32, 64, 48, 48]               0\n",
            "            Conv2d-5           [32, 32, 48, 48]          18,464\n",
            "              ReLU-6           [32, 32, 48, 48]               0\n",
            "        DenseLayer-7           [32, 96, 48, 48]               0\n",
            "            Conv2d-8           [32, 32, 48, 48]          27,680\n",
            "              ReLU-9           [32, 32, 48, 48]               0\n",
            "       DenseLayer-10          [32, 128, 48, 48]               0\n",
            "           Conv2d-11           [32, 32, 48, 48]          36,896\n",
            "             ReLU-12           [32, 32, 48, 48]               0\n",
            "       DenseLayer-13          [32, 160, 48, 48]               0\n",
            "           Conv2d-14           [32, 32, 48, 48]           5,152\n",
            "================================================================\n",
            "Total params: 99,520\n",
            "Trainable params: 99,520\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 18.00\n",
            "Forward/backward pass size (MB): 432.00\n",
            "Params size (MB): 0.38\n",
            "Estimated Total Size (MB): 450.38\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IucZWalWKytC"
      },
      "source": [
        "# Channel attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH_jMQhqK0vz"
      },
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "  \"\"\"Base Channel-Wise Attention module\n",
        "  \n",
        "    Args:\n",
        "      in_channels: Input channels\n",
        "      reduction: Reduction to use for compressing channels information ( channels_reduced = in_channels // reduction )\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(self, in_channels : int , reduction : int):\n",
        "    super(ChannelAttention,self).__init__()\n",
        "\n",
        "    channels_reduced = in_channels // reduction\n",
        "\n",
        "    \n",
        "    self.attention = nn.Sequential(\n",
        "        # max pooling\n",
        "        nn.AdaptiveMaxPool2d(output_size=1),\n",
        "\n",
        "        # apply reduction\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=channels_reduced, kernel_size = 1),\n",
        "    \n",
        "        nn.ReLU(),\n",
        "\n",
        "        # remove reduction\n",
        "        nn.Conv2d(in_channels=channels_reduced, out_channels=in_channels, kernel_size = 1),\n",
        "\n",
        "        nn.Sigmoid()           \n",
        "    )\n",
        "\n",
        "  def forward(self,input : Tensor) -> Tensor:\n",
        "    attention_mask = self.attention(input)\n",
        "    return input * attention_mask"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MxmEyA8YUoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64c6018-e7a2-4c6a-e764-17ec8eb1c67b"
      },
      "source": [
        "#TEST ChannelAttention\n",
        "CA = ChannelAttention(64,reduction = 16)\n",
        "\n",
        "input_size = (64,48,48)\n",
        "\n",
        "summary(CA,input_size, device=\"cpu\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            " AdaptiveMaxPool2d-1             [-1, 64, 1, 1]               0\n",
            "            Conv2d-2              [-1, 4, 1, 1]             260\n",
            "              ReLU-3              [-1, 4, 1, 1]               0\n",
            "            Conv2d-4             [-1, 64, 1, 1]             320\n",
            "           Sigmoid-5             [-1, 64, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 580\n",
            "Trainable params: 580\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.56\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.57\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KdBMuiYY2YT"
      },
      "source": [
        "# Dense Attention Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iWT4V2ZY5Rz"
      },
      "source": [
        "def DenseAttentionBlock(in_channels : int , out_channels : int , intra_layer_output_features : int, n_intra_layers : int, reduction : int):\n",
        "  '''Base Dense Attention Block used in the paper\n",
        "\n",
        "  Args:\n",
        "    in_channels : Input channels of the Dense Attention Block\n",
        "    out_channels : Output channels of the Dense Attention Block ( which is equal to out_compressed_channels: output channels of the compression convolutions (at the beginning and at the end of IntraDenseBlock) ( see :class:`~IntraDenseBlock` ) )\n",
        "    intra_layer_output_features: Output channels of each convolution inside the block  ( see :class:`~IntraDenseBlock` )\n",
        "    n_intra_layers: Number of convolutions inside the block  ( see :class:`~IntraDenseBlock` )\n",
        "    reduction: Reduction to use for compressing channels information ( channels_reduced = in_channels // reduction )  ( see :class:`~ChannelAttention` )\n",
        "  '''\n",
        "  \n",
        "  return nn.Sequential(\n",
        "      IntraDenseBlock(in_channels=in_channels, out_compressed_channels=out_channels, intra_layer_output_features=intra_layer_output_features, n_intra_layers=n_intra_layers),\n",
        "      ChannelAttention(in_channels=out_channels, reduction = reduction)\n",
        "      )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqs9mQe_afiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2804669a-cfc1-45c9-9762-39bceb7d95ee"
      },
      "source": [
        "# Test DAB\n",
        "DAB = DenseAttentionBlock(in_channels=64,out_channels=64,intra_layer_output_features=64,n_intra_layers=8,reduction=16)\n",
        "summary(DAB,(64,48,48),batch_size = 32, device=\"cpu\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [32, 64, 48, 48]           4,160\n",
            "            Conv2d-2           [32, 64, 48, 48]          36,928\n",
            "              ReLU-3           [32, 64, 48, 48]               0\n",
            "        DenseLayer-4          [32, 128, 48, 48]               0\n",
            "            Conv2d-5           [32, 64, 48, 48]          73,792\n",
            "              ReLU-6           [32, 64, 48, 48]               0\n",
            "        DenseLayer-7          [32, 192, 48, 48]               0\n",
            "            Conv2d-8           [32, 64, 48, 48]         110,656\n",
            "              ReLU-9           [32, 64, 48, 48]               0\n",
            "       DenseLayer-10          [32, 256, 48, 48]               0\n",
            "           Conv2d-11           [32, 64, 48, 48]         147,520\n",
            "             ReLU-12           [32, 64, 48, 48]               0\n",
            "       DenseLayer-13          [32, 320, 48, 48]               0\n",
            "           Conv2d-14           [32, 64, 48, 48]         184,384\n",
            "             ReLU-15           [32, 64, 48, 48]               0\n",
            "       DenseLayer-16          [32, 384, 48, 48]               0\n",
            "           Conv2d-17           [32, 64, 48, 48]         221,248\n",
            "             ReLU-18           [32, 64, 48, 48]               0\n",
            "       DenseLayer-19          [32, 448, 48, 48]               0\n",
            "           Conv2d-20           [32, 64, 48, 48]         258,112\n",
            "             ReLU-21           [32, 64, 48, 48]               0\n",
            "       DenseLayer-22          [32, 512, 48, 48]               0\n",
            "           Conv2d-23           [32, 64, 48, 48]         294,976\n",
            "             ReLU-24           [32, 64, 48, 48]               0\n",
            "       DenseLayer-25          [32, 576, 48, 48]               0\n",
            "           Conv2d-26           [32, 64, 48, 48]          36,928\n",
            "  IntraDenseBlock-27           [32, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-28             [32, 64, 1, 1]               0\n",
            "           Conv2d-29              [32, 4, 1, 1]             260\n",
            "             ReLU-30              [32, 4, 1, 1]               0\n",
            "           Conv2d-31             [32, 64, 1, 1]             320\n",
            "          Sigmoid-32             [32, 64, 1, 1]               0\n",
            " ChannelAttention-33           [32, 64, 48, 48]               0\n",
            "================================================================\n",
            "Total params: 1,369,284\n",
            "Trainable params: 1,369,284\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 18.00\n",
            "Forward/backward pass size (MB): 2304.05\n",
            "Params size (MB): 5.22\n",
            "Estimated Total Size (MB): 2327.27\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dds1mzsdkLYv"
      },
      "source": [
        "# FMB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQAJm2bNzkp-"
      },
      "source": [
        "class FeatureMappingBranch(nn.Module):\n",
        "  \"\"\" Feature Extraction Branch\n",
        "  Args:\n",
        "    in_channels: Input image channel(s) (1:grayscale, 3:rgb)\n",
        "    low_level_features: Output channels of the low level features extraction convolution\n",
        "    n_dab: Number of Dense Attention Blocks\n",
        "    out_channels_dab: Output channels of the Dense Attention Block ( which is equal to out_compressed_channels: output channels of the compression convolutions (at the beginning and at the end of IntraDenseBlock) ( see :class:`~IntraDenseBlock` ) )\n",
        "    intra_layers_output_features: Output channels of convolutions inside the Intra Dense Block ( see :class:`~IntraDenseBlock` )\n",
        "    n_intra_layers: Number of convolutions inside Intra Dense Block ( see :class:`~IntraDenseBlock` )\n",
        "    reduction: Reduction applied inside the Channel Attention layer ( see :class:`~ChannelAttention` )\n",
        "\n",
        "  Attributes:\n",
        "    final_output: the final output channels after the concatenation of all previous channels\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels, low_level_features, n_dab, out_channels_dab, intra_layer_output_features, n_intra_layers, reduction):\n",
        "    super(FeatureMappingBranch,self).__init__()\n",
        "    \n",
        "    self.final_output = low_level_features + n_dab * out_channels_dab\n",
        "\n",
        "    # extract low level features from the input\n",
        "    self.low_level_features = nn.Conv2d(in_channels=in_channels,\n",
        "                                        out_channels=low_level_features,\n",
        "                                        kernel_size = 3, \n",
        "                                        stride = 1,\n",
        "                                        padding = 1)\n",
        "    \n",
        "    # Dense attention blocks\n",
        "    self.n_dab = n_dab\n",
        "    dabs = list()\n",
        "    for n in range(n_dab):\n",
        "      block_input_channels = low_level_features + n * out_channels_dab\n",
        "      dabs.append(\n",
        "          DenseLayer(\n",
        "          DenseAttentionBlock(in_channels=block_input_channels,\n",
        "                              out_channels=out_channels_dab,\n",
        "                              intra_layer_output_features=intra_layer_output_features,\n",
        "                              n_intra_layers=n_intra_layers,\n",
        "                              reduction=reduction)))\n",
        "    \n",
        "    self.dabs = nn.Sequential(*dabs)\n",
        "                                 \n",
        "  def forward(self,input):\n",
        "    low_level_features = self.low_level_features(input)\n",
        "    if input.requires_grad:\n",
        "      dense_output = torch.utils.checkpoint.checkpoint_sequential(self.dabs,2,low_level_features)\n",
        "    else:\n",
        "      dense_output = self.dabs(low_level_features)\n",
        "    \n",
        "    return dense_output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs-nd7TUczyR",
        "outputId": "e572add3-3f95-4ec9-8723-7db07aa8177f"
      },
      "source": [
        "# Test FEB\n",
        "FEB = FeatureMappingBranch(in_channels=64,low_level_features=64,n_dab=16,out_channels_dab=64,intra_layer_output_features=64,n_intra_layers=8,reduction=16)\n",
        "summary(FEB,(64,48,48),device=\"cpu\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 48, 48]          36,928\n",
            "            Conv2d-2           [-1, 64, 48, 48]           4,160\n",
            "            Conv2d-3           [-1, 64, 48, 48]          36,928\n",
            "              ReLU-4           [-1, 64, 48, 48]               0\n",
            "        DenseLayer-5          [-1, 128, 48, 48]               0\n",
            "            Conv2d-6           [-1, 64, 48, 48]          73,792\n",
            "              ReLU-7           [-1, 64, 48, 48]               0\n",
            "        DenseLayer-8          [-1, 192, 48, 48]               0\n",
            "            Conv2d-9           [-1, 64, 48, 48]         110,656\n",
            "             ReLU-10           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-11          [-1, 256, 48, 48]               0\n",
            "           Conv2d-12           [-1, 64, 48, 48]         147,520\n",
            "             ReLU-13           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-14          [-1, 320, 48, 48]               0\n",
            "           Conv2d-15           [-1, 64, 48, 48]         184,384\n",
            "             ReLU-16           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-17          [-1, 384, 48, 48]               0\n",
            "           Conv2d-18           [-1, 64, 48, 48]         221,248\n",
            "             ReLU-19           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-20          [-1, 448, 48, 48]               0\n",
            "           Conv2d-21           [-1, 64, 48, 48]         258,112\n",
            "             ReLU-22           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-23          [-1, 512, 48, 48]               0\n",
            "           Conv2d-24           [-1, 64, 48, 48]         294,976\n",
            "             ReLU-25           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-26          [-1, 576, 48, 48]               0\n",
            "           Conv2d-27           [-1, 64, 48, 48]          36,928\n",
            "  IntraDenseBlock-28           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-29             [-1, 64, 1, 1]               0\n",
            "           Conv2d-30              [-1, 4, 1, 1]             260\n",
            "             ReLU-31              [-1, 4, 1, 1]               0\n",
            "           Conv2d-32             [-1, 64, 1, 1]             320\n",
            "          Sigmoid-33             [-1, 64, 1, 1]               0\n",
            " ChannelAttention-34           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-35          [-1, 128, 48, 48]               0\n",
            "           Conv2d-36           [-1, 64, 48, 48]           8,256\n",
            "           Conv2d-37           [-1, 64, 48, 48]          36,928\n",
            "             ReLU-38           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-39          [-1, 128, 48, 48]               0\n",
            "           Conv2d-40           [-1, 64, 48, 48]          73,792\n",
            "             ReLU-41           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-42          [-1, 192, 48, 48]               0\n",
            "           Conv2d-43           [-1, 64, 48, 48]         110,656\n",
            "             ReLU-44           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-45          [-1, 256, 48, 48]               0\n",
            "           Conv2d-46           [-1, 64, 48, 48]         147,520\n",
            "             ReLU-47           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-48          [-1, 320, 48, 48]               0\n",
            "           Conv2d-49           [-1, 64, 48, 48]         184,384\n",
            "             ReLU-50           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-51          [-1, 384, 48, 48]               0\n",
            "           Conv2d-52           [-1, 64, 48, 48]         221,248\n",
            "             ReLU-53           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-54          [-1, 448, 48, 48]               0\n",
            "           Conv2d-55           [-1, 64, 48, 48]         258,112\n",
            "             ReLU-56           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-57          [-1, 512, 48, 48]               0\n",
            "           Conv2d-58           [-1, 64, 48, 48]         294,976\n",
            "             ReLU-59           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-60          [-1, 576, 48, 48]               0\n",
            "           Conv2d-61           [-1, 64, 48, 48]          36,928\n",
            "  IntraDenseBlock-62           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-63             [-1, 64, 1, 1]               0\n",
            "           Conv2d-64              [-1, 4, 1, 1]             260\n",
            "             ReLU-65              [-1, 4, 1, 1]               0\n",
            "           Conv2d-66             [-1, 64, 1, 1]             320\n",
            "          Sigmoid-67             [-1, 64, 1, 1]               0\n",
            " ChannelAttention-68           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-69          [-1, 192, 48, 48]               0\n",
            "           Conv2d-70           [-1, 64, 48, 48]          12,352\n",
            "           Conv2d-71           [-1, 64, 48, 48]          36,928\n",
            "             ReLU-72           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-73          [-1, 128, 48, 48]               0\n",
            "           Conv2d-74           [-1, 64, 48, 48]          73,792\n",
            "             ReLU-75           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-76          [-1, 192, 48, 48]               0\n",
            "           Conv2d-77           [-1, 64, 48, 48]         110,656\n",
            "             ReLU-78           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-79          [-1, 256, 48, 48]               0\n",
            "           Conv2d-80           [-1, 64, 48, 48]         147,520\n",
            "             ReLU-81           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-82          [-1, 320, 48, 48]               0\n",
            "           Conv2d-83           [-1, 64, 48, 48]         184,384\n",
            "             ReLU-84           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-85          [-1, 384, 48, 48]               0\n",
            "           Conv2d-86           [-1, 64, 48, 48]         221,248\n",
            "             ReLU-87           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-88          [-1, 448, 48, 48]               0\n",
            "           Conv2d-89           [-1, 64, 48, 48]         258,112\n",
            "             ReLU-90           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-91          [-1, 512, 48, 48]               0\n",
            "           Conv2d-92           [-1, 64, 48, 48]         294,976\n",
            "             ReLU-93           [-1, 64, 48, 48]               0\n",
            "       DenseLayer-94          [-1, 576, 48, 48]               0\n",
            "           Conv2d-95           [-1, 64, 48, 48]          36,928\n",
            "  IntraDenseBlock-96           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-97             [-1, 64, 1, 1]               0\n",
            "           Conv2d-98              [-1, 4, 1, 1]             260\n",
            "             ReLU-99              [-1, 4, 1, 1]               0\n",
            "          Conv2d-100             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-101             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-102           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-103          [-1, 256, 48, 48]               0\n",
            "          Conv2d-104           [-1, 64, 48, 48]          16,448\n",
            "          Conv2d-105           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-106           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-107          [-1, 128, 48, 48]               0\n",
            "          Conv2d-108           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-109           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-110          [-1, 192, 48, 48]               0\n",
            "          Conv2d-111           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-112           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-113          [-1, 256, 48, 48]               0\n",
            "          Conv2d-114           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-115           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-116          [-1, 320, 48, 48]               0\n",
            "          Conv2d-117           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-118           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-119          [-1, 384, 48, 48]               0\n",
            "          Conv2d-120           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-121           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-122          [-1, 448, 48, 48]               0\n",
            "          Conv2d-123           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-124           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-125          [-1, 512, 48, 48]               0\n",
            "          Conv2d-126           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-127           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-128          [-1, 576, 48, 48]               0\n",
            "          Conv2d-129           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-130           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-131             [-1, 64, 1, 1]               0\n",
            "          Conv2d-132              [-1, 4, 1, 1]             260\n",
            "            ReLU-133              [-1, 4, 1, 1]               0\n",
            "          Conv2d-134             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-135             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-136           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-137          [-1, 320, 48, 48]               0\n",
            "          Conv2d-138           [-1, 64, 48, 48]          20,544\n",
            "          Conv2d-139           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-140           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-141          [-1, 128, 48, 48]               0\n",
            "          Conv2d-142           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-143           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-144          [-1, 192, 48, 48]               0\n",
            "          Conv2d-145           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-146           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-147          [-1, 256, 48, 48]               0\n",
            "          Conv2d-148           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-149           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-150          [-1, 320, 48, 48]               0\n",
            "          Conv2d-151           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-152           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-153          [-1, 384, 48, 48]               0\n",
            "          Conv2d-154           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-155           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-156          [-1, 448, 48, 48]               0\n",
            "          Conv2d-157           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-158           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-159          [-1, 512, 48, 48]               0\n",
            "          Conv2d-160           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-161           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-162          [-1, 576, 48, 48]               0\n",
            "          Conv2d-163           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-164           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-165             [-1, 64, 1, 1]               0\n",
            "          Conv2d-166              [-1, 4, 1, 1]             260\n",
            "            ReLU-167              [-1, 4, 1, 1]               0\n",
            "          Conv2d-168             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-169             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-170           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-171          [-1, 384, 48, 48]               0\n",
            "          Conv2d-172           [-1, 64, 48, 48]          24,640\n",
            "          Conv2d-173           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-174           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-175          [-1, 128, 48, 48]               0\n",
            "          Conv2d-176           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-177           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-178          [-1, 192, 48, 48]               0\n",
            "          Conv2d-179           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-180           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-181          [-1, 256, 48, 48]               0\n",
            "          Conv2d-182           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-183           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-184          [-1, 320, 48, 48]               0\n",
            "          Conv2d-185           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-186           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-187          [-1, 384, 48, 48]               0\n",
            "          Conv2d-188           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-189           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-190          [-1, 448, 48, 48]               0\n",
            "          Conv2d-191           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-192           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-193          [-1, 512, 48, 48]               0\n",
            "          Conv2d-194           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-195           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-196          [-1, 576, 48, 48]               0\n",
            "          Conv2d-197           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-198           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-199             [-1, 64, 1, 1]               0\n",
            "          Conv2d-200              [-1, 4, 1, 1]             260\n",
            "            ReLU-201              [-1, 4, 1, 1]               0\n",
            "          Conv2d-202             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-203             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-204           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-205          [-1, 448, 48, 48]               0\n",
            "          Conv2d-206           [-1, 64, 48, 48]          28,736\n",
            "          Conv2d-207           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-208           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-209          [-1, 128, 48, 48]               0\n",
            "          Conv2d-210           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-211           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-212          [-1, 192, 48, 48]               0\n",
            "          Conv2d-213           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-214           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-215          [-1, 256, 48, 48]               0\n",
            "          Conv2d-216           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-217           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-218          [-1, 320, 48, 48]               0\n",
            "          Conv2d-219           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-220           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-221          [-1, 384, 48, 48]               0\n",
            "          Conv2d-222           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-223           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-224          [-1, 448, 48, 48]               0\n",
            "          Conv2d-225           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-226           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-227          [-1, 512, 48, 48]               0\n",
            "          Conv2d-228           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-229           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-230          [-1, 576, 48, 48]               0\n",
            "          Conv2d-231           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-232           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-233             [-1, 64, 1, 1]               0\n",
            "          Conv2d-234              [-1, 4, 1, 1]             260\n",
            "            ReLU-235              [-1, 4, 1, 1]               0\n",
            "          Conv2d-236             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-237             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-238           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-239          [-1, 512, 48, 48]               0\n",
            "          Conv2d-240           [-1, 64, 48, 48]          32,832\n",
            "          Conv2d-241           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-242           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-243          [-1, 128, 48, 48]               0\n",
            "          Conv2d-244           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-245           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-246          [-1, 192, 48, 48]               0\n",
            "          Conv2d-247           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-248           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-249          [-1, 256, 48, 48]               0\n",
            "          Conv2d-250           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-251           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-252          [-1, 320, 48, 48]               0\n",
            "          Conv2d-253           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-254           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-255          [-1, 384, 48, 48]               0\n",
            "          Conv2d-256           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-257           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-258          [-1, 448, 48, 48]               0\n",
            "          Conv2d-259           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-260           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-261          [-1, 512, 48, 48]               0\n",
            "          Conv2d-262           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-263           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-264          [-1, 576, 48, 48]               0\n",
            "          Conv2d-265           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-266           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-267             [-1, 64, 1, 1]               0\n",
            "          Conv2d-268              [-1, 4, 1, 1]             260\n",
            "            ReLU-269              [-1, 4, 1, 1]               0\n",
            "          Conv2d-270             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-271             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-272           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-273          [-1, 576, 48, 48]               0\n",
            "          Conv2d-274           [-1, 64, 48, 48]          36,928\n",
            "          Conv2d-275           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-276           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-277          [-1, 128, 48, 48]               0\n",
            "          Conv2d-278           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-279           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-280          [-1, 192, 48, 48]               0\n",
            "          Conv2d-281           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-282           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-283          [-1, 256, 48, 48]               0\n",
            "          Conv2d-284           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-285           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-286          [-1, 320, 48, 48]               0\n",
            "          Conv2d-287           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-288           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-289          [-1, 384, 48, 48]               0\n",
            "          Conv2d-290           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-291           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-292          [-1, 448, 48, 48]               0\n",
            "          Conv2d-293           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-294           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-295          [-1, 512, 48, 48]               0\n",
            "          Conv2d-296           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-297           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-298          [-1, 576, 48, 48]               0\n",
            "          Conv2d-299           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-300           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-301             [-1, 64, 1, 1]               0\n",
            "          Conv2d-302              [-1, 4, 1, 1]             260\n",
            "            ReLU-303              [-1, 4, 1, 1]               0\n",
            "          Conv2d-304             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-305             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-306           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-307          [-1, 640, 48, 48]               0\n",
            "          Conv2d-308           [-1, 64, 48, 48]          41,024\n",
            "          Conv2d-309           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-310           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-311          [-1, 128, 48, 48]               0\n",
            "          Conv2d-312           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-313           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-314          [-1, 192, 48, 48]               0\n",
            "          Conv2d-315           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-316           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-317          [-1, 256, 48, 48]               0\n",
            "          Conv2d-318           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-319           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-320          [-1, 320, 48, 48]               0\n",
            "          Conv2d-321           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-322           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-323          [-1, 384, 48, 48]               0\n",
            "          Conv2d-324           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-325           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-326          [-1, 448, 48, 48]               0\n",
            "          Conv2d-327           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-328           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-329          [-1, 512, 48, 48]               0\n",
            "          Conv2d-330           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-331           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-332          [-1, 576, 48, 48]               0\n",
            "          Conv2d-333           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-334           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-335             [-1, 64, 1, 1]               0\n",
            "          Conv2d-336              [-1, 4, 1, 1]             260\n",
            "            ReLU-337              [-1, 4, 1, 1]               0\n",
            "          Conv2d-338             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-339             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-340           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-341          [-1, 704, 48, 48]               0\n",
            "          Conv2d-342           [-1, 64, 48, 48]          45,120\n",
            "          Conv2d-343           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-344           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-345          [-1, 128, 48, 48]               0\n",
            "          Conv2d-346           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-347           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-348          [-1, 192, 48, 48]               0\n",
            "          Conv2d-349           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-350           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-351          [-1, 256, 48, 48]               0\n",
            "          Conv2d-352           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-353           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-354          [-1, 320, 48, 48]               0\n",
            "          Conv2d-355           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-356           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-357          [-1, 384, 48, 48]               0\n",
            "          Conv2d-358           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-359           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-360          [-1, 448, 48, 48]               0\n",
            "          Conv2d-361           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-362           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-363          [-1, 512, 48, 48]               0\n",
            "          Conv2d-364           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-365           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-366          [-1, 576, 48, 48]               0\n",
            "          Conv2d-367           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-368           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-369             [-1, 64, 1, 1]               0\n",
            "          Conv2d-370              [-1, 4, 1, 1]             260\n",
            "            ReLU-371              [-1, 4, 1, 1]               0\n",
            "          Conv2d-372             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-373             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-374           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-375          [-1, 768, 48, 48]               0\n",
            "          Conv2d-376           [-1, 64, 48, 48]          49,216\n",
            "          Conv2d-377           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-378           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-379          [-1, 128, 48, 48]               0\n",
            "          Conv2d-380           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-381           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-382          [-1, 192, 48, 48]               0\n",
            "          Conv2d-383           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-384           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-385          [-1, 256, 48, 48]               0\n",
            "          Conv2d-386           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-387           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-388          [-1, 320, 48, 48]               0\n",
            "          Conv2d-389           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-390           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-391          [-1, 384, 48, 48]               0\n",
            "          Conv2d-392           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-393           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-394          [-1, 448, 48, 48]               0\n",
            "          Conv2d-395           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-396           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-397          [-1, 512, 48, 48]               0\n",
            "          Conv2d-398           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-399           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-400          [-1, 576, 48, 48]               0\n",
            "          Conv2d-401           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-402           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-403             [-1, 64, 1, 1]               0\n",
            "          Conv2d-404              [-1, 4, 1, 1]             260\n",
            "            ReLU-405              [-1, 4, 1, 1]               0\n",
            "          Conv2d-406             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-407             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-408           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-409          [-1, 832, 48, 48]               0\n",
            "          Conv2d-410           [-1, 64, 48, 48]          53,312\n",
            "          Conv2d-411           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-412           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-413          [-1, 128, 48, 48]               0\n",
            "          Conv2d-414           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-415           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-416          [-1, 192, 48, 48]               0\n",
            "          Conv2d-417           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-418           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-419          [-1, 256, 48, 48]               0\n",
            "          Conv2d-420           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-421           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-422          [-1, 320, 48, 48]               0\n",
            "          Conv2d-423           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-424           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-425          [-1, 384, 48, 48]               0\n",
            "          Conv2d-426           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-427           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-428          [-1, 448, 48, 48]               0\n",
            "          Conv2d-429           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-430           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-431          [-1, 512, 48, 48]               0\n",
            "          Conv2d-432           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-433           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-434          [-1, 576, 48, 48]               0\n",
            "          Conv2d-435           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-436           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-437             [-1, 64, 1, 1]               0\n",
            "          Conv2d-438              [-1, 4, 1, 1]             260\n",
            "            ReLU-439              [-1, 4, 1, 1]               0\n",
            "          Conv2d-440             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-441             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-442           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-443          [-1, 896, 48, 48]               0\n",
            "          Conv2d-444           [-1, 64, 48, 48]          57,408\n",
            "          Conv2d-445           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-446           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-447          [-1, 128, 48, 48]               0\n",
            "          Conv2d-448           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-449           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-450          [-1, 192, 48, 48]               0\n",
            "          Conv2d-451           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-452           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-453          [-1, 256, 48, 48]               0\n",
            "          Conv2d-454           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-455           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-456          [-1, 320, 48, 48]               0\n",
            "          Conv2d-457           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-458           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-459          [-1, 384, 48, 48]               0\n",
            "          Conv2d-460           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-461           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-462          [-1, 448, 48, 48]               0\n",
            "          Conv2d-463           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-464           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-465          [-1, 512, 48, 48]               0\n",
            "          Conv2d-466           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-467           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-468          [-1, 576, 48, 48]               0\n",
            "          Conv2d-469           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-470           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-471             [-1, 64, 1, 1]               0\n",
            "          Conv2d-472              [-1, 4, 1, 1]             260\n",
            "            ReLU-473              [-1, 4, 1, 1]               0\n",
            "          Conv2d-474             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-475             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-476           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-477          [-1, 960, 48, 48]               0\n",
            "          Conv2d-478           [-1, 64, 48, 48]          61,504\n",
            "          Conv2d-479           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-480           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-481          [-1, 128, 48, 48]               0\n",
            "          Conv2d-482           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-483           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-484          [-1, 192, 48, 48]               0\n",
            "          Conv2d-485           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-486           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-487          [-1, 256, 48, 48]               0\n",
            "          Conv2d-488           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-489           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-490          [-1, 320, 48, 48]               0\n",
            "          Conv2d-491           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-492           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-493          [-1, 384, 48, 48]               0\n",
            "          Conv2d-494           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-495           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-496          [-1, 448, 48, 48]               0\n",
            "          Conv2d-497           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-498           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-499          [-1, 512, 48, 48]               0\n",
            "          Conv2d-500           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-501           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-502          [-1, 576, 48, 48]               0\n",
            "          Conv2d-503           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-504           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-505             [-1, 64, 1, 1]               0\n",
            "          Conv2d-506              [-1, 4, 1, 1]             260\n",
            "            ReLU-507              [-1, 4, 1, 1]               0\n",
            "          Conv2d-508             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-509             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-510           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-511         [-1, 1024, 48, 48]               0\n",
            "          Conv2d-512           [-1, 64, 48, 48]          65,600\n",
            "          Conv2d-513           [-1, 64, 48, 48]          36,928\n",
            "            ReLU-514           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-515          [-1, 128, 48, 48]               0\n",
            "          Conv2d-516           [-1, 64, 48, 48]          73,792\n",
            "            ReLU-517           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-518          [-1, 192, 48, 48]               0\n",
            "          Conv2d-519           [-1, 64, 48, 48]         110,656\n",
            "            ReLU-520           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-521          [-1, 256, 48, 48]               0\n",
            "          Conv2d-522           [-1, 64, 48, 48]         147,520\n",
            "            ReLU-523           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-524          [-1, 320, 48, 48]               0\n",
            "          Conv2d-525           [-1, 64, 48, 48]         184,384\n",
            "            ReLU-526           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-527          [-1, 384, 48, 48]               0\n",
            "          Conv2d-528           [-1, 64, 48, 48]         221,248\n",
            "            ReLU-529           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-530          [-1, 448, 48, 48]               0\n",
            "          Conv2d-531           [-1, 64, 48, 48]         258,112\n",
            "            ReLU-532           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-533          [-1, 512, 48, 48]               0\n",
            "          Conv2d-534           [-1, 64, 48, 48]         294,976\n",
            "            ReLU-535           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-536          [-1, 576, 48, 48]               0\n",
            "          Conv2d-537           [-1, 64, 48, 48]          36,928\n",
            " IntraDenseBlock-538           [-1, 64, 48, 48]               0\n",
            "AdaptiveMaxPool2d-539             [-1, 64, 1, 1]               0\n",
            "          Conv2d-540              [-1, 4, 1, 1]             260\n",
            "            ReLU-541              [-1, 4, 1, 1]               0\n",
            "          Conv2d-542             [-1, 64, 1, 1]             320\n",
            "         Sigmoid-543             [-1, 64, 1, 1]               0\n",
            "ChannelAttention-544           [-1, 64, 48, 48]               0\n",
            "      DenseLayer-545         [-1, 1088, 48, 48]               0\n",
            "================================================================\n",
            "Total params: 22,436,992\n",
            "Trainable params: 22,436,992\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.56\n",
            "Forward/backward pass size (MB): 1324.15\n",
            "Params size (MB): 85.59\n",
            "Estimated Total Size (MB): 1410.30\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQs-O17KkOUp"
      },
      "source": [
        "# IRB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdaa9WZcRKvZ"
      },
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "  def __init__(self,in_channels, expansion ):\n",
        "    super(SpatialAttention,self).__init__()\n",
        "    \n",
        "    self.sa = nn.Sequential(\n",
        "        # expand the channels for each activation\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=expansion * in_channels, kernel_size=1, stride=1,padding=0),\n",
        "\n",
        "        # extract information at each spatial location\n",
        "        nn.ReLU(),\n",
        "\n",
        "        # restore the original channels dimension at each activation\n",
        "        nn.Conv2d(in_channels=in_channels * expansion, out_channels=in_channels,kernel_size = 1,stride=1,padding=0),\n",
        "\n",
        "        # create the spatial-attention mask\n",
        "        nn.Sigmoid()\n",
        "        )\n",
        "  \n",
        "  def forward(self,input):\n",
        "    mask = self.sa(input)\n",
        "    return input * mask"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnu4ptxbLtZE"
      },
      "source": [
        "class ImageReconstructionBranch(nn.Module):\n",
        "  \"\"\" Image Reconstruction Branch defined inside the paper \n",
        "  Args:\n",
        "    in_channels: Input channels of the Image Reconstruction Branch ( which is the output channels of the Feature Extraction Branch )\n",
        "    out_channels: Input image channel(s) (1:grayscale, 3:rgb)\n",
        "    expansion: How much expand the channels inside the Spatial Attention for extracting high level frequency spatial information\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels, out_channels, expansion):\n",
        "    super(ImageReconstructionBranch,self).__init__()\n",
        "    \n",
        "    self.transform_into_image = nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "    self.sa = SpatialAttention(out_channels, expansion)\n",
        "\n",
        "  def forward(self, input_image, input_features):\n",
        "    activations_to_image = self.transform_into_image(input_features)\n",
        "\n",
        "    high_frequency_spatial_information = self.sa(activations_to_image)\n",
        "    \n",
        "    return input_image + high_frequency_spatial_information"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZMIHJH8Uaz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa681ad0-7369-4a65-9444-834a94e41b6f"
      },
      "source": [
        "low_level_features = 64\n",
        "n_dab = 16\n",
        "out_dab = 64\n",
        "\n",
        "output_feature_extractor = low_level_features + n_dab * out_dab\n",
        "image_channels = 3\n",
        "image_size = 48\n",
        "expansion = 2\n",
        "scale = 1.5\n",
        "\n",
        "# TODO: Interpolated before or after\n",
        "import math\n",
        "# interpolated_image = nn.functional.interpolate(input_image, scale_factor=self.scale,mode=\"bicubic\", align_corners=False) \n",
        "interpolate_input_image_size = math.floor(image_size * scale)\n",
        "interpolated_input = (3, interpolate_input_image_size, interpolate_input_image_size)\n",
        "\n",
        "IRB = ImageReconstructionBranch(output_feature_extractor, image_channels,expansion)\n",
        "summary(IRB,[interpolated_input,(output_feature_extractor,interpolate_input_image_size,interpolate_input_image_size)],batch_size = 32, device=\"cpu\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [32, 3, 72, 72]           3,267\n",
            "            Conv2d-2            [32, 6, 72, 72]              24\n",
            "              ReLU-3            [32, 6, 72, 72]               0\n",
            "            Conv2d-4            [32, 3, 72, 72]              21\n",
            "           Sigmoid-5            [32, 3, 72, 72]               0\n",
            "  SpatialAttention-6            [32, 3, 72, 72]               0\n",
            "================================================================\n",
            "Total params: 3,312\n",
            "Trainable params: 3,312\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 10707552.00\n",
            "Forward/backward pass size (MB): 30.38\n",
            "Params size (MB): 0.01\n",
            "Estimated Total Size (MB): 10707582.39\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVq13fWskQkO"
      },
      "source": [
        "# ASDN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSgzwJISN5rs"
      },
      "source": [
        "from collections import namedtuple\n",
        "import math\n",
        "\n",
        "Level = namedtuple(\"Level\", \"index scale size\")\n",
        "\n",
        "class LaplacianFrequencyRepresentation:\n",
        "  def __init__(self, start,end,count, patch_size):\n",
        "    \n",
        "    self.count = count\n",
        "    # if numpy is availablbe use np.linspace(start,end,count)\n",
        "    step = (end - start) / (count - 1) \n",
        "    scales = [start + l*step for l in range(count)]\n",
        "    sizes  = [math.floor(patch_size * scale) for scale in scales]\n",
        "    self.information = [ Level(index,scale,size) for index,(scale,size) in enumerate(zip(scales,sizes)) ]\n",
        "\n",
        "  def get_index(self, scale):\n",
        "    \"\"\" Return the corresponding pyramid index for the given decimal scale\n",
        "    Args:\n",
        "      scale: Decimal scale\n",
        "    Returns:\n",
        "      index\n",
        "    \"\"\"\n",
        "    return math.ceil((self.count - 1) * (scale - 1))\n",
        "\n",
        "  def get_weight(self,scale):\n",
        "    \"\"\" Return the interpolation weight for the given decimal scale\n",
        "    Args:\n",
        "      scale: Decimal scale\n",
        "    Returns:\n",
        "      weight\n",
        "    \"\"\"\n",
        "    return (self.count - 1) * (self.information[self.get_index(scale)].scale - scale)\n",
        "\n",
        "  def get_for(self,scale):\n",
        "    index = self.get_index(scale)\n",
        "    return self.information[index-1], self.information[index] \n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq5WklrSVIAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975dc82f-1f01-4bbf-e5cb-cb61822a9383"
      },
      "source": [
        "lpr = LaplacianFrequencyRepresentation(1,2,11,48)\n",
        "for info in lpr.information:\n",
        "  print(info)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Level(index=0, scale=1.0, size=48)\n",
            "Level(index=1, scale=1.1, size=52)\n",
            "Level(index=2, scale=1.2, size=57)\n",
            "Level(index=3, scale=1.3, size=62)\n",
            "Level(index=4, scale=1.4, size=67)\n",
            "Level(index=5, scale=1.5, size=72)\n",
            "Level(index=6, scale=1.6, size=76)\n",
            "Level(index=7, scale=1.7000000000000002, size=81)\n",
            "Level(index=8, scale=1.8, size=86)\n",
            "Level(index=9, scale=1.9, size=91)\n",
            "Level(index=10, scale=2.0, size=96)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozv-lcBVdS6l"
      },
      "source": [
        "class ASDNNetwork(nn.Module):\n",
        "  def __init__(self, pyramid_levels : int, input_image_channels : int, **FMB_kwargs):\n",
        "    super(ASDNNetwork,self).__init__()\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Rename FEB into FMB\n",
        "    # TODO: Custom paramters for FeatureEXtractionBranch\n",
        "\n",
        "    # Use custom parameters if available otherwise relay on ones defined in the paper\n",
        "    self.feature_mapping_branch = FeatureMappingBranch(in_channels=FMB_kwargs.get(\"in_channels\",3),\n",
        "                                                          low_level_features=FMB_kwargs.get(\"low_level_features\",64),\n",
        "                                                          n_dab=FMB_kwargs.get(\"n_dab\",16),\n",
        "                                                          out_channels_dab=FMB_kwargs.get(\"out_compressed_channels\",64),\n",
        "                                                          intra_layer_output_features=FMB_kwargs.get(\"intra_layer_output_features\",64),\n",
        "                                                          n_intra_layers=FMB_kwargs.get(\"n_intra_layers\",8),\n",
        "                                                          reduction=FMB_kwargs.get(\"reduction\",16))\n",
        "    \n",
        "    # change this if you are going to change the feature extractor\n",
        "    self.in_channels_irb = self.feature_mapping_branch.final_output\n",
        "    # and also this\n",
        "    # expansion = 2 inside the paper [9]\n",
        "    self.image_reconstruction_branches = nn.ModuleList([ImageReconstructionBranch(self.in_channels_irb,input_image_channels,2) for _ in range(pyramid_levels)])\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, interpolated_patch, irb_index):\n",
        "    features_extracted = self.feature_mapping_branch(interpolated_patch)\n",
        "    \n",
        "    output_leveli = self.image_reconstruction_branches[irb_index](interpolated_patch, features_extracted)\n",
        "  \n",
        "    return output_leveli"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VMav0_pV2kP"
      },
      "source": [
        "from functools import partial\n",
        "lfr = LaplacianFrequencyRepresentation(1,2,11,48)\n",
        "leveliminus1, leveli = lfr.get_for(scale = 1.3674)\n",
        "\n",
        "#ASDN = ASDNNetwork(lfr.count,3, n_dab=8, n_intra_layers=8).to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#ASDN.forward = partial(ASDN.forward, irb_index = leveli.index)\n",
        "#summary(ASDN,(3,48,48),device=\"cuda\" if torch.cuda.is_available() else \"cpu\" ,batch_size = 16)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Su51_ZkTnT"
      },
      "source": [
        "# Test ASDN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gtno8oLfaiy",
        "outputId": "925407d6-50ae-4bda-c0e4-7aedb322816e"
      },
      "source": [
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "\n",
        "cpu = \"cpu\"\n",
        "gpu = \"cuda:0\"\n",
        "\n",
        "scale = 1.3674\n",
        "lfr = LaplacianFrequencyRepresentation(1,2,11,48)\n",
        "leveliminus1, leveli = lfr.get_for(scale = scale)\n",
        "\n",
        "class RandomTensorDataset:\n",
        "  def __init__(self):\n",
        "    self.patch = (3,48,48)\n",
        "    self.len = 64\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return torch.rand(*self.patch)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "# batch size 8 use 5.5GB \n",
        "loader = DataLoader(RandomTensorDataset(),batch_size=16)\n",
        "\n",
        "ASDN = ASDNNetwork(lfr.count,3)\n",
        "ASDN = ASDN.to(gpu)\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "ASDN.train()\n",
        "for index,batch in enumerate(loader):\n",
        "\n",
        "    # TODO: Move interpolation inside collate_fn\n",
        "    start.record()\n",
        "    interpolatedi = nn.functional.interpolate(batch,scale_factor=leveli.scale, mode=\"bicubic\").to(gpu)\n",
        "    outputi = ASDN(interpolatedi,irb_index=leveli.index).to(cpu)\n",
        "\n",
        "\n",
        "    interpolatedi = nn.functional.interpolate(batch,scale_factor=leveliminus1.scale, mode=\"bicubic\").to(gpu)\n",
        "    outputiminus1 = ASDN(interpolatedi,irb_index=leveli.index).to(cpu)\n",
        "    end.record()\n",
        "\n",
        "    print(\"=\"*3, index, \"=\"*3)\n",
        "    print(f\"Time passed (s) {start.elapsed_time(end) / 1000:.2f}\")\n",
        "\n",
        "    print(outputi.size())  \n",
        "    print(outputiminus1.size()) "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "=== 0 ===\n",
            "Time passed (s) 0.69\n",
            "torch.Size([16, 3, 67, 67])\n",
            "torch.Size([16, 3, 62, 62])\n",
            "=== 1 ===\n",
            "Time passed (s) 0.69\n",
            "torch.Size([16, 3, 67, 67])\n",
            "torch.Size([16, 3, 62, 62])\n",
            "=== 2 ===\n",
            "Time passed (s) 0.69\n",
            "torch.Size([16, 3, 67, 67])\n",
            "torch.Size([16, 3, 62, 62])\n",
            "=== 3 ===\n",
            "Time passed (s) 0.69\n",
            "torch.Size([16, 3, 67, 67])\n",
            "torch.Size([16, 3, 62, 62])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7-3nSGwotpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbb8857-ad7e-4815-bb07-8e8a6d829678"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec  8 07:35:28 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    32W / 250W |  14449MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}